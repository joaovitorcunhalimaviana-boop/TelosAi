"""
Script para treinar modelo ML com dados COLETIVOS pseudonimizados
Usa API Next.js para buscar dataset anonimizado de m√©dicos participantes
"""

import pandas as pd
import requests
import os
from dotenv import load_dotenv
from model import ComplicationPredictor

# Carrega vari√°veis de ambiente
load_dotenv("../.env")

NEXTAUTH_URL = os.getenv("NEXTAUTH_URL", "http://localhost:3000")
ADMIN_API_KEY = os.getenv("ADMIN_API_KEY")  # API key do admin


def fetch_collective_dataset():
    """
    Busca dataset pseudonimizado da API Next.js
    Apenas admin pode fazer isso
    """
    print("üîó Buscando dataset coletivo pseudonimizado...")

    url = f"{NEXTAUTH_URL}/api/collective-intelligence/export-dataset"

    headers = {}
    if ADMIN_API_KEY:
        headers["Authorization"] = f"Bearer {ADMIN_API_KEY}"

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        data = response.json()

        if not data.get("success"):
            print(f"‚ùå Erro: {data.get('message', 'Erro desconhecido')}")
            return None

        dataset = data["dataset"]
        stats = data.get("stats", {})

        print(f"‚úÖ Dataset carregado com sucesso!")
        print(f"   M√©dicos participantes: {stats.get('participatingDoctors', 0)}")
        print(f"   Pacientes eleg√≠veis: {stats.get('eligiblePatients', 0)}")
        print(f"   Total de cirurgias: {dataset['totalSurgeries']}")
        print(f"   Total de follow-ups: {dataset['totalFollowUps']}")

        return dataset

    except requests.RequestException as e:
        print(f"‚ùå Erro ao buscar dataset: {e}")
        return None


def convert_to_dataframe(dataset):
    """
    Converte dataset pseudonimizado para DataFrame
    """
    print("\nüìä Convertendo dataset para formato tabular...")

    rows = []

    for patient in dataset["patients"]:
        age = patient["age"]
        sex = patient["sex"]
        comorbidities = ",".join(patient["comorbidities"]) if patient["comorbidities"] else ""
        comorbidity_count = len(patient["comorbidities"])

        # Processa cada cirurgia
        for surgery in patient["surgeries"]:
            surgery_type = surgery["type"]
            duration = surgery["durationMinutes"]
            pudendal_block = 1 if surgery["pudendalBlock"] else 0

            # Processa cada follow-up
            for followup in patient["followUps"]:
                if followup["day"] == 1:  # Apenas D+1 para features
                    row = {
                        "idade": age,
                        "sexo": sex,
                        "comorbidades": comorbidities,
                        "tipo_cirurgia": surgery_type,
                        "duracao_minutos": duration,
                        "bloqueio_pudendo": pudendal_block,
                        "dor_d1": followup["painLevel"],
                        "retencao_urinaria": 1 if followup["urinaryRetention"] else 0,
                        "febre": 1 if followup["fever"] else 0,
                        "sangramento_intenso": 1 if followup["bleeding"] else 0,
                        "teve_complicacao": 1 if followup["hasComplications"] else 0,
                    }
                    rows.append(row)

    df = pd.DataFrame(rows)

    # Remove linhas com dados faltantes cr√≠ticos
    df = df.dropna(subset=["idade", "tipo_cirurgia", "dor_d1"])

    print(f"‚úÖ DataFrame criado: {len(df)} amostras")

    return df


def main():
    """
    Fun√ß√£o principal de treinamento com dados coletivos
    """
    print("=" * 60)
    print("ü§ñ TREINAMENTO COM INTELIG√äNCIA COLETIVA")
    print("=" * 60)
    print("\nüìã Este script usa dados PSEUDONIMIZADOS de:")
    print("   ‚úì M√©dicos que optaram por participar")
    print("   ‚úì Pacientes com termo de consentimento assinado")
    print("   ‚úì Dados anonimizados (SHA-256)")
    print("   ‚úì Conforme LGPD (Art. 7¬∫, IV e Art. 11)")
    print()

    # 1. Busca dataset coletivo
    dataset = fetch_collective_dataset()

    if not dataset or dataset["totalPatients"] == 0:
        print("\n‚ùå Sem dados para treinamento.")
        print("   Certifique-se de que:")
        print("   1. M√©dicos optaram por participar (collectiveIntelligenceOptIn)")
        print("   2. Pacientes assinaram termo de consentimento")
        print("   3. Voc√™ est√° autenticado como admin")
        return

    # 2. Converte para DataFrame
    df = convert_to_dataframe(dataset)

    if len(df) < 30:
        print("\n‚ö†Ô∏è ATEN√á√ÉO: Poucos dados para treinamento!")
        print(f"   M√≠nimo recomendado: 100 amostras")
        print(f"   Voc√™ tem: {len(df)} amostras")
        print()

        response = input("Deseja continuar mesmo assim? (s/n): ")
        if response.lower() != "s":
            print("‚ùå Treinamento cancelado.")
            return

    # 3. An√°lise explorat√≥ria
    print("\nüìà AN√ÅLISE EXPLORAT√ìRIA DOS DADOS COLETIVOS:")
    print(f"Total de amostras: {len(df)}")
    print(
        f"Complica√ß√µes: {df['teve_complicacao'].sum()} ({df['teve_complicacao'].sum()/len(df)*100:.1f}%)"
    )
    print(f"Idade m√©dia: {df['idade'].mean():.1f} anos (¬±{df['idade'].std():.1f})")
    print(f"Dor D+1 m√©dia: {df['dor_d1'].mean():.1f}/10 (¬±{df['dor_d1'].std():.1f})")

    print("\nDistribui√ß√£o por tipo de cirurgia:")
    print(df["tipo_cirurgia"].value_counts())

    print("\nDistribui√ß√£o por sexo:")
    print(df["sexo"].value_counts())

    # 4. Treina modelo Random Forest
    print("\n" + "=" * 60)
    print("üå≤ RANDOM FOREST (Dados Coletivos)")
    print("=" * 60)

    predictor_rf = ComplicationPredictor(model_type="random_forest")
    metrics_rf = predictor_rf.train(df)
    predictor_rf.save("models/complication_predictor_collective_rf.joblib")

    # 5. Treina modelo Gradient Boosting
    print("\n" + "=" * 60)
    print("‚ö° GRADIENT BOOSTING (Dados Coletivos)")
    print("=" * 60)

    predictor_gb = ComplicationPredictor(model_type="gradient_boosting")
    metrics_gb = predictor_gb.train(df)
    predictor_gb.save("models/complication_predictor_collective_gb.joblib")

    # 6. Compara modelos
    print("\n" + "=" * 60)
    print("üèÜ COMPARA√á√ÉO DE MODELOS")
    print("=" * 60)

    comparison = pd.DataFrame(
        {"Random Forest": metrics_rf, "Gradient Boosting": metrics_gb}
    ).T

    print(comparison)

    # Escolhe melhor modelo
    if metrics_rf["roc_auc"] >= metrics_gb["roc_auc"]:
        print("\n‚úÖ VENCEDOR: Random Forest")
        print(f"   AUC-ROC: {metrics_rf['roc_auc']:.3f}")

        # Salva como modelo padr√£o COLETIVO
        predictor_rf.save("models/complication_predictor_collective.joblib")
        best_model = "Random Forest"
        best_auc = metrics_rf["roc_auc"]
    else:
        print("\n‚úÖ VENCEDOR: Gradient Boosting")
        print(f"   AUC-ROC: {metrics_gb['roc_auc']:.3f}")

        # Salva como modelo padr√£o COLETIVO
        predictor_gb.save("models/complication_predictor_collective.joblib")
        best_model = "Gradient Boosting"
        best_auc = metrics_gb["roc_auc"]

    print("\n" + "=" * 60)
    print("‚úÖ TREINAMENTO COM INTELIG√äNCIA COLETIVA CONCLU√çDO!")
    print("=" * 60)
    print(f"\nüéØ Melhor modelo: {best_model}")
    print(f"üìä AUC-ROC: {best_auc:.3f}")
    print(f"üìÅ Salvo em: models/complication_predictor_collective.joblib")
    print(f"\nüìà Dados utilizados:")
    print(f"   ‚Ä¢ {dataset['totalPatients']} pacientes pseudonimizados")
    print(f"   ‚Ä¢ {dataset['totalSurgeries']} cirurgias")
    print(f"   ‚Ä¢ {dataset['totalFollowUps']} follow-ups")
    print(f"   ‚Ä¢ De m√∫ltiplos m√©dicos participantes")
    print(f"\nüîí Privacidade:")
    print(f"   ‚Ä¢ M√©todo: {dataset['metadata']['pseudonymizationMethod']}")
    print(f"   ‚Ä¢ LGPD Compliant: {dataset['metadata']['lgpdCompliant']}")

    print("\nüìù Pr√≥ximos passos:")
    print("1. Compare com modelo individual (se existir)")
    print("2. Inicie a API: python api.py")
    print("3. Modelo coletivo est√° dispon√≠vel para TODOS os m√©dicos participantes")
    print("4. Benef√≠cio: Maior precis√£o com dados de diferentes perfis")


if __name__ == "__main__":
    main()
